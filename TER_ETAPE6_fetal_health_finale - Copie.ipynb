{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                    APRENTISSAGE SEMI-SUPERVISE \n",
    "###                                               (DATASET DE CLASSIFICATION : FETAL_HEALTH DE KAGGLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAB (10%), non-LAB (80%), TEST (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________\n",
      "LAB (10%), non-LAB (80%), TEST (10%)  : \n",
      "                                                 \n",
      "-LAB : (382, 21)\n",
      "-non_LAB: (1531, 21)\n",
      "-TEST : (213, 21)\n",
      "_________________________________________________\n",
      "Score de prédiction :  0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#Bibliothèques\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "#from sklearn.metrics import zero_one_loss, accuracy_score,precision_score, recall_score, f1_score\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 1 - Téléchargement et traitement du dataset fetal_health \n",
    "data = pd.read_csv('fetal_health.csv')\n",
    "data.shape\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(data)\n",
    "data=imp.transform(data) \n",
    "NB_FEATURES=21\n",
    "X=data[:,0:NB_FEATURES]         # Features: inputs (21 premières colonnes)\n",
    "y=data[:,NB_FEATURES:].ravel()  # Target : output/ (Dernière colonne)\n",
    "\n",
    "\n",
    "# 2 - Préparation du dataset semi-supervisé\n",
    "# Découpage Train/Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1, stratify=y)\n",
    "# Découpage LAB/non-LAB \n",
    "X_train_lab, X_train_unlab, y_train_lab, y_train_unlab = train_test_split(X_train, y_train, test_size=0.80, random_state=1, stratify=y_train)\n",
    "#Ensemble de trainig 90% (10% LAB/ 80% non-LAB)\n",
    "print(\"_________________________________________________\")\n",
    "print(\"LAB (10%), non-LAB (80%), TEST (10%)  : \")\n",
    "print(\"                                                 \")\n",
    "print('-LAB :', X_train_lab.shape)\n",
    "print('-non_LAB:', X_train_unlab.shape)\n",
    "#Ensemble de test 10%\n",
    "print('-TEST :', X_test.shape)\n",
    "\n",
    "\n",
    "# 3 - Normalisation\n",
    "#scaler=preprocessing.StandardScaler().fit(X_train)\n",
    "#X_train=scaler.transform(X_train)\n",
    "#X_test=scaler.transform(X_test)\n",
    "\n",
    "# Définition d'un modele MLP (avec paramètres par défaut car il a donné déja de bons résultats)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), activation='relu',  solver='adam',\n",
    "                   alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001,\n",
    "                   power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False,\n",
    "                   warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False,\n",
    "                   validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, \n",
    "                   )\n",
    "\n",
    "\n",
    "# Entrainement du modele sur le dataset labelisé\n",
    "mlp.fit(X_train_lab, y_train_lab)\n",
    "\n",
    "\n",
    "#Self Trainig\n",
    "mlp.predict_proba(X_test[:])\n",
    "mlp.predict(X_test[:, :])\n",
    "\n",
    "#Métrique \n",
    "print(\"_________________________________________________\")\n",
    "print(\"Score de prédiction : \",mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAB (20%), non-LAB (70%), TEST (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________\n",
      "LAB (20%), non-LAB (70%), TEST (10%)  : \n",
      "                                                 \n",
      "-LAB : (573, 21)\n",
      "-non_LAB: (1340, 21)\n",
      "-TEST : (213, 21)\n",
      "_________________________________________________\n",
      "Score de prédiction :  0.863849765258216\n"
     ]
    }
   ],
   "source": [
    "#Bibliothèques\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "#from sklearn.metrics import zero_one_loss, accuracy_score,precision_score, recall_score, f1_score\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 1 - Téléchargement et traitement du dataset fetal_health \n",
    "data = pd.read_csv('fetal_health.csv')\n",
    "data.shape\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(data)\n",
    "data=imp.transform(data) \n",
    "NB_FEATURES=21\n",
    "X=data[:,0:NB_FEATURES]         # Features: inputs (21 premières colonnes)\n",
    "y=data[:,NB_FEATURES:].ravel()  # Target : output/ (Dernière colonne)\n",
    "\n",
    "# 2 - Préparation du dataset semi-supervisé\n",
    "# Découpage Train/Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1, stratify=y)\n",
    "# Découpage LAB/non-LAB \n",
    "X_train_lab, X_train_unlab, y_train_lab, y_train_unlab = train_test_split(X_train, y_train, test_size=0.7, random_state=1, stratify=y_train)\n",
    "#Ensemble de trainig 90% (20% LAB/ 70% non-LAB)\n",
    "print(\"_________________________________________________\")\n",
    "print(\"LAB (20%), non-LAB (70%), TEST (10%)  : \")\n",
    "print(\"                                                 \")\n",
    "print('-LAB :', X_train_lab.shape)\n",
    "print('-non_LAB:', X_train_unlab.shape)\n",
    "#Ensemble de test 10%\n",
    "print('-TEST :', X_test.shape)\n",
    "\n",
    "\n",
    "# 3 - Normalisation\n",
    "#scaler=preprocessing.StandardScaler().fit(X_train)\n",
    "#X_train=scaler.transform(X_train)\n",
    "#X_test=scaler.transform(X_test)\n",
    "\n",
    "# Définition d'un modele MLP (avec paramètres par défaut car il a donné déja de bons résultats)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), activation='relu',  solver='adam',\n",
    "                   alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001,\n",
    "                   power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False,\n",
    "                   warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False,\n",
    "                   validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, \n",
    "                   )\n",
    "\n",
    "\n",
    "# Entrainement du modele sur le dataset labelisé\n",
    "mlp.fit(X_train_lab, y_train_lab)\n",
    "\n",
    "\n",
    "#Self Trainig\n",
    "mlp.predict_proba(X_test[:])\n",
    "mlp.predict(X_test[:, :])\n",
    "\n",
    "#Métrique \n",
    "print(\"_________________________________________________\")\n",
    "print(\"Score de prédiction : \",mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAB (30%), non-LAB (60%), TEST (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________\n",
      "LAB (30%), non-LAB (60%), TEST (10%)  : \n",
      "                                                 \n",
      "-LAB : (765, 21)\n",
      "-non_LAB: (1148, 21)\n",
      "-TEST : (213, 21)\n",
      "_________________________________________________\n",
      "Score de prédiction :  0.863849765258216\n"
     ]
    }
   ],
   "source": [
    "#Bibliothèques\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "#from sklearn.metrics import zero_one_loss, accuracy_score,precision_score, recall_score, f1_score\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 1 - Téléchargement et traitement du dataset fetal_health \n",
    "data = pd.read_csv('fetal_health.csv')\n",
    "data.shape\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(data)\n",
    "data=imp.transform(data) \n",
    "NB_FEATURES=21\n",
    "X=data[:,0:NB_FEATURES]         # Features: inputs (21 premières colonnes)\n",
    "y=data[:,NB_FEATURES:].ravel()  # Target : output/ (Dernière colonne)\n",
    "\n",
    "# 2 - Préparation du dataset semi-supervisé\n",
    "# Découpage Train/Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1, stratify=y)\n",
    "# Découpage LAB/non-LAB \n",
    "X_train_lab, X_train_unlab, y_train_lab, y_train_unlab = train_test_split(X_train, y_train, test_size=0.60, random_state=1, stratify=y_train)\n",
    "#Ensemble de trainig 90% (30% LAB/ 60% non-LAB)\n",
    "print(\"_________________________________________________\")\n",
    "print(\"LAB (30%), non-LAB (60%), TEST (10%)  : \")\n",
    "print(\"                                                 \")\n",
    "print('-LAB :', X_train_lab.shape)\n",
    "print('-non_LAB:', X_train_unlab.shape)\n",
    "#Ensemble de test 10%\n",
    "print('-TEST :', X_test.shape)\n",
    "\n",
    "\n",
    "# 3 - Normalisation\n",
    "#scaler=preprocessing.StandardScaler().fit(X_train)\n",
    "#X_train=scaler.transform(X_train)\n",
    "#X_test=scaler.transform(X_test)\n",
    "\n",
    "# Définition d'un modele MLP (avec paramètres par défaut car il a donné déja de bons résultats)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), activation='relu',  solver='adam',\n",
    "                   alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001,\n",
    "                   power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False,\n",
    "                   warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False,\n",
    "                   validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, \n",
    "                   )\n",
    "\n",
    "\n",
    "# Entrainement du modele sur le dataset labelisé\n",
    "mlp.fit(X_train_lab, y_train_lab)\n",
    "\n",
    "\n",
    "#Self Trainig\n",
    "mlp.predict_proba(X_test[:])\n",
    "mlp.predict(X_test[:, :])\n",
    "\n",
    "#Métrique \n",
    "print(\"_________________________________________________\")\n",
    "print(\"Score de prédiction : \",mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAB (40%), non-LAB (50%), TEST (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________\n",
      "LAB (40%), non-LAB (50%), TEST (10%)  : \n",
      "                                                 \n",
      "-LAB : (956, 21)\n",
      "-non_LAB: (957, 21)\n",
      "-TEST : (213, 21)\n",
      "_________________________________________________\n",
      "Score de prédiction :  0.8544600938967136\n"
     ]
    }
   ],
   "source": [
    "#Bibliothèques\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "#from sklearn.metrics import zero_one_loss, accuracy_score,precision_score, recall_score, f1_score\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 1 - Téléchargement et traitement du dataset fetal_health \n",
    "data = pd.read_csv('fetal_health.csv')\n",
    "data.shape\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(data)\n",
    "data=imp.transform(data) \n",
    "NB_FEATURES=21\n",
    "X=data[:,0:NB_FEATURES]         # Features: inputs (21 premières colonnes)\n",
    "y=data[:,NB_FEATURES:].ravel()  # Target : output/ (Dernière colonne)\n",
    "\n",
    "# 2 - Préparation du dataset semi-supervisé\n",
    "# Découpage Train/Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1, stratify=y)\n",
    "# Découpage LAB/non-LAB \n",
    "X_train_lab, X_train_unlab, y_train_lab, y_train_unlab = train_test_split(X_train, y_train, test_size=0.50, random_state=1, stratify=y_train)\n",
    "#Ensemble de trainig 90% (40% LAB/ 50% non-LAB)\n",
    "print(\"_________________________________________________\")\n",
    "print(\"LAB (40%), non-LAB (50%), TEST (10%)  : \")\n",
    "print(\"                                                 \")\n",
    "print('-LAB :', X_train_lab.shape)\n",
    "print('-non_LAB:', X_train_unlab.shape)\n",
    "#Ensemble de test 10%\n",
    "print('-TEST :', X_test.shape)\n",
    "\n",
    "\n",
    "# 3 - Normalisation\n",
    "#scaler=preprocessing.StandardScaler().fit(X_train)\n",
    "#X_train=scaler.transform(X_train)\n",
    "#X_test=scaler.transform(X_test)\n",
    "\n",
    "# Définition d'un modele MLP (avec paramètres par défaut car il a donné déja de bons résultats)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), activation='relu',  solver='adam',\n",
    "                   alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001,\n",
    "                   power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False,\n",
    "                   warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False,\n",
    "                   validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, \n",
    "                   )\n",
    "\n",
    "\n",
    "# Entrainement du modele sur le dataset labelisé\n",
    "mlp.fit(X_train_lab, y_train_lab)\n",
    "\n",
    "\n",
    "#Self Trainig\n",
    "mlp.predict_proba(X_test[:])\n",
    "mlp.predict(X_test[:, :])\n",
    "\n",
    "#Métrique\n",
    "print(\"_________________________________________________\")\n",
    "print(\"Score de prédiction : \",mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAB (50%), non-LAB (40%), TEST (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________\n",
      "LAB (50%), non-LAB (40%), TEST (10%)  : \n",
      "                                                 \n",
      "-LAB : (1147, 21)\n",
      "-non_LAB: (766, 21)\n",
      "-TEST : (213, 21)\n",
      "_________________________________________________\n",
      "Score de prédiction :  0.8544600938967136\n"
     ]
    }
   ],
   "source": [
    "#Bibliothèques\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "#from sklearn.metrics import zero_one_loss, accuracy_score,precision_score, recall_score, f1_score\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 1 - Téléchargement et traitement du dataset fetal_health \n",
    "data = pd.read_csv('fetal_health.csv')\n",
    "data.shape\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(data)\n",
    "data=imp.transform(data) \n",
    "NB_FEATURES=21\n",
    "X=data[:,0:NB_FEATURES]         # Features: inputs (21 premières colonnes)\n",
    "y=data[:,NB_FEATURES:].ravel()  # Target : output/ (Dernière colonne)\n",
    "\n",
    "# 2 - Préparation du dataset semi-supervisé\n",
    "# Découpage Train/Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1, stratify=y)\n",
    "# Découpage LAB/non-LAB \n",
    "X_train_lab, X_train_unlab, y_train_lab, y_train_unlab = train_test_split(X_train, y_train, test_size=0.40, random_state=1, stratify=y_train)\n",
    "#Ensemble de trainig 90% (50% LAB/ 40% non-LAB)\n",
    "print(\"_________________________________________________\")\n",
    "print(\"LAB (50%), non-LAB (40%), TEST (10%)  : \")\n",
    "print(\"                                                 \")\n",
    "print('-LAB :', X_train_lab.shape)\n",
    "print('-non_LAB:', X_train_unlab.shape)\n",
    "#Ensemble de test 10%\n",
    "print('-TEST :', X_test.shape)\n",
    "\n",
    "\n",
    "# 3 - Normalisation\n",
    "#scaler=preprocessing.StandardScaler().fit(X_train)\n",
    "#X_train=scaler.transform(X_train)\n",
    "#X_test=scaler.transform(X_test)\n",
    "\n",
    "# Définition d'un modele MLP (avec paramètres par défaut car il a donné déja de bons résultats)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), activation='relu',  solver='adam',\n",
    "                   alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001,\n",
    "                   power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False,\n",
    "                   warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False,\n",
    "                   validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, \n",
    "                   )\n",
    "\n",
    "\n",
    "# Entrainement du modele sur le dataset labelisé\n",
    "mlp.fit(X_train_lab, y_train_lab)\n",
    "\n",
    "\n",
    "#Self Trainig\n",
    "mlp.predict_proba(X_test[:])\n",
    "mlp.predict(X_test[:, :])\n",
    "\n",
    "#Métrique\n",
    "print(\"_________________________________________________\")\n",
    "print(\"Score de prédiction : \",mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAB (60%), non-LAB (30%), TEST (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________\n",
      "LAB (60%), non-LAB (30%), TEST (10%)  : \n",
      "                                                 \n",
      "-LAB : (1339, 21)\n",
      "-non_LAB: (574, 21)\n",
      "-TEST : (213, 21)\n",
      "_________________________________________________\n",
      "Score de prédiction :  0.8591549295774648\n"
     ]
    }
   ],
   "source": [
    "#Bibliothèques\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "#from sklearn.metrics import zero_one_loss, accuracy_score,precision_score, recall_score, f1_score\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 1 - Téléchargement et traitement du dataset fetal_health \n",
    "data = pd.read_csv('fetal_health.csv')\n",
    "data.shape\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(data)\n",
    "data=imp.transform(data) \n",
    "NB_FEATURES=21\n",
    "X=data[:,0:NB_FEATURES]         # Features: inputs (21 premières colonnes)\n",
    "y=data[:,NB_FEATURES:].ravel()  # Target : output/ (Dernière colonne)\n",
    "\n",
    "# 2 - Préparation du dataset semi-supervisé\n",
    "# Découpage Train/Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1, stratify=y)\n",
    "# Découpage LAB/non-LAB \n",
    "X_train_lab, X_train_unlab, y_train_lab, y_train_unlab = train_test_split(X_train, y_train, test_size=0.30, random_state=1, stratify=y_train)\n",
    "#Ensemble de trainig 90% (60% LAB/ 30% non-LAB)\n",
    "print(\"_________________________________________________\")\n",
    "print(\"LAB (60%), non-LAB (30%), TEST (10%)  : \")\n",
    "print(\"                                                 \")\n",
    "print('-LAB :', X_train_lab.shape)\n",
    "print('-non_LAB:', X_train_unlab.shape)\n",
    "#Ensemble de test 10%\n",
    "print('-TEST :', X_test.shape)\n",
    "\n",
    "\n",
    "# 3 - Normalisation\n",
    "#scaler=preprocessing.StandardScaler().fit(X_train)\n",
    "#X_train=scaler.transform(X_train)\n",
    "#X_test=scaler.transform(X_test)\n",
    "\n",
    "# Définition d'un modele MLP (avec paramètres par défaut car il a donné déja de bons résultats)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), activation='relu',  solver='adam',\n",
    "                   alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001,\n",
    "                   power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False,\n",
    "                   warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False,\n",
    "                   validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, \n",
    "                   )\n",
    "\n",
    "\n",
    "# Entrainement du modele sur le dataset labelisé\n",
    "mlp.fit(X_train_lab, y_train_lab)\n",
    "\n",
    "\n",
    "#Self Trainig\n",
    "mlp.predict_proba(X_test[:])\n",
    "mlp.predict(X_test[:, :])\n",
    "\n",
    "#Métrique\n",
    "print(\"_________________________________________________\")\n",
    "print(\"Score de prédiction : \",mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAB (70%), non-LAB (20%), TEST (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________\n",
      "LAB (70%), non-LAB (20%), TEST (10%)  : \n",
      "                                                 \n",
      "-LAB : (1530, 21)\n",
      "-non_LAB: (383, 21)\n",
      "-TEST : (213, 21)\n",
      "_________________________________________________\n",
      "Score de prédiction :  0.8544600938967136\n"
     ]
    }
   ],
   "source": [
    "#Bibliothèques\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "#from sklearn.metrics import zero_one_loss, accuracy_score,precision_score, recall_score, f1_score\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 1 - Téléchargement et traitement du dataset fetal_health \n",
    "data = pd.read_csv('fetal_health.csv')\n",
    "data.shape\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(data)\n",
    "data=imp.transform(data) \n",
    "NB_FEATURES=21\n",
    "X=data[:,0:NB_FEATURES]         # Features: inputs (21 premières colonnes)\n",
    "y=data[:,NB_FEATURES:].ravel()  # Target : output/ (Dernière colonne)\n",
    "\n",
    "\n",
    "# 2 - Préparation du dataset semi-supervisé\n",
    "# Découpage Train/Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1, stratify=y)\n",
    "# Découpage LAB/non-LAB \n",
    "X_train_lab, X_train_unlab, y_train_lab, y_train_unlab = train_test_split(X_train, y_train, test_size=0.20, random_state=1, stratify=y_train)\n",
    "#Ensemble de trainig 90% (70% LAB/ 20% non-LAB)\n",
    "print(\"_________________________________________________\")\n",
    "print(\"LAB (70%), non-LAB (20%), TEST (10%)  : \")\n",
    "print(\"                                                 \")\n",
    "print('-LAB :', X_train_lab.shape)\n",
    "print('-non_LAB:', X_train_unlab.shape)\n",
    "#Ensemble de test 10%\n",
    "print('-TEST :', X_test.shape)\n",
    "\n",
    "\n",
    "# 3 - Normalisation\n",
    "#scaler=preprocessing.StandardScaler().fit(X_train)\n",
    "#X_train=scaler.transform(X_train)\n",
    "#X_test=scaler.transform(X_test)\n",
    "\n",
    "# Définition d'un modele MLP (avec paramètres par défaut car il a donné déja de bons résultats)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), activation='relu',  solver='adam',\n",
    "                   alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001,\n",
    "                   power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False,\n",
    "                   warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False,\n",
    "                   validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, \n",
    "                   )\n",
    "\n",
    "\n",
    "# Entrainement du modele sur le dataset labelisé\n",
    "mlp.fit(X_train_lab, y_train_lab)\n",
    "\n",
    "\n",
    "#Self Trainig\n",
    "mlp.predict_proba(X_test[:])\n",
    "mlp.predict(X_test[:, :])\n",
    "\n",
    "#Métrique\n",
    "print(\"_________________________________________________\")\n",
    "print(\"Score de prédiction : \",mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAB (80%), non-LAB (10%), TEST (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________\n",
      "LAB (80%), non-LAB (10%), TEST (10%)  : \n",
      "                                                 \n",
      "-LAB : (1721, 21)\n",
      "-non_LAB: (192, 21)\n",
      "-TEST : (213, 21)\n",
      "_________________________________________________\n",
      "Score de prédiction :  0.8544600938967136\n"
     ]
    }
   ],
   "source": [
    "#Bibliothèques\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "#from sklearn.metrics import zero_one_loss, accuracy_score,precision_score, recall_score, f1_score\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 1 - Téléchargement et traitement du dataset fetal_health \n",
    "data = pd.read_csv('fetal_health.csv')\n",
    "data.shape\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(data)\n",
    "data=imp.transform(data) \n",
    "NB_FEATURES=21\n",
    "X=data[:,0:NB_FEATURES]         # Features: inputs (21 premières colonnes)\n",
    "y=data[:,NB_FEATURES:].ravel()  # Target : output/ (Dernière colonne)\n",
    "\n",
    "\n",
    "# 2 - Préparation du dataset semi-supervisé\n",
    "# Découpage Train/Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1, stratify=y)\n",
    "# Découpage LAB/non-LAB\n",
    "X_train_lab, X_train_unlab, y_train_lab, y_train_unlab = train_test_split(X_train, y_train, test_size=0.10, random_state=1, stratify=y_train)\n",
    "#Ensemble de trainig 90% (80% LAB/ 10% non-LAB)\n",
    "print(\"_________________________________________________\")\n",
    "print(\"LAB (80%), non-LAB (10%), TEST (10%)  : \")\n",
    "print(\"                                                 \")\n",
    "print('-LAB :', X_train_lab.shape)\n",
    "print('-non_LAB:', X_train_unlab.shape)\n",
    "#Ensemble de test 10%\n",
    "print('-TEST :', X_test.shape)\n",
    "\n",
    "\n",
    "# 3 - Normalisation\n",
    "#scaler=preprocessing.StandardScaler().fit(X_train)\n",
    "#X_train=scaler.transform(X_train)\n",
    "#X_test=scaler.transform(X_test)\n",
    "\n",
    "# Définition d'un modele MLP (avec paramètres par défaut car il a donné déja de bons résultats)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), activation='relu',  solver='adam',\n",
    "                   alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001,\n",
    "                   power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False,\n",
    "                   warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False,\n",
    "                   validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, \n",
    "                   )\n",
    "\n",
    "\n",
    "# Entrainement du modele sur le dataset labelisé\n",
    "mlp.fit(X_train_lab, y_train_lab)\n",
    "\n",
    "\n",
    "#Self Trainig\n",
    "mlp.predict_proba(X_test[:])\n",
    "mlp.predict(X_test[:, :])\n",
    "\n",
    "#Métrique\n",
    "print(\"_________________________________________________\")\n",
    "print(\"Score de prédiction : \",mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variation de la métrique (score de prédiction) en fonction des proportions LAB/non-LAB \n",
    "#### PS : résultats copiés de l'Excel\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\tLAB (10%) / non-LAB (80%)\tLAB (20%) / non-LAB (70%)\tLAB (30%) / non-LAB (60%)\tLAB (40%) / non-LAB (50%)\tLAB (50%) / non-LAB (40%)\tLAB (60%) / non-LAB (30%)\tLAB (70%) / non-LAB (20%)\tLAB (80%) / non-LAB (10%)\n",
    "Score de prédiction\t0,88\t0,863849765\t0,863849765\t0,854460094\t0,854460094\t0,85915493\t0,854460094\t0,854460094\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB (90%), non-LAB (0%), TEST (10%)\n",
    "##### Il s'agit de l'apprentissage supervisé (étape 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB (0%), non-LAB (90%), TEST (10%)\n",
    "##### Il s'agit de l'apprentissage non-supervisé (Machines Boltzmann restreintes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
