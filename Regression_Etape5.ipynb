{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Bibliothèques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Chargement de données des attributs pertinents (4/7 seulement) depuis le fichier 'insurance.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1338, 4)\n",
      "Index(['age', 'bmi', 'children', 'charges'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('insurance.csv')\n",
    "data.columns\n",
    "data_sample = data[['age','bmi', 'children', 'charges']]\n",
    "print (data_sample.shape)\n",
    "print (data_sample.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.  Correction de données manquantes : exemple de complétion de ces valeurs par la moyenne de la colonne\n",
    "PS : Nous n'avons pas de valeurs manquantes donc ce bout de code ne va pas influencer notre code ou données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(data_sample)\n",
    "data=imp.transform(data_sample) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Séprataion des attributs (Inputs) et de l'attribut objectif (Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_FEATURES=3\n",
    "X=data[:,0:NB_FEATURES]         # Features: inputs (3 premières colonnes)\n",
    "y=data[:,NB_FEATURES:].ravel()  # Target : output/ (Dernière colonne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Découpage de l'ensemble de données : 80% TRAINING, 20% TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Affihage des quelques informations concernant notre dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >>> Taille du dataset / nombre d'instances :\t 1338 \n",
      " >>> Nombre d'attributs / Inputs : \t\t 3 \n",
      " >>> Taille du dataset d'entrainement : \t 1070\n",
      " >>> Taille du dataset de test : \t\t 268\n"
     ]
    }
   ],
   "source": [
    "print(f\" >>> Taille du dataset / nombre d'instances :\\t {data.shape[0]} \")\n",
    "print(f\" >>> Nombre d'attributs / Inputs : \\t\\t {NB_FEATURES} \")\n",
    "print(f\" >>> Taille du dataset d'entrainement : \\t {X_train.shape[0]}\")\n",
    "print(f\" >>> Taille du dataset de test : \\t\\t {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Normalisation des données du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=preprocessing.StandardScaler().fit(X_train)\n",
    "X_train=scaler.transform(X_train)\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etape 2 : Paramètres par défaut\n",
    "#### 8. Création d'un modèle de regression avec les paramètres par défaut de ScikitLearn + apprentissage du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "regr = MLPRegressor(hidden_layer_sizes=(100,), activation='relu',  solver='adam',\n",
    "                   alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001,\n",
    "                   power_t=0.5, max_iter=200, shuffle=True, random_state=1, tol=0.0001, verbose=False,\n",
    "                   warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False,\n",
    "                   validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, \n",
    "                   )\n",
    "\n",
    "regr.fit(X_train, y_train)\n",
    "y_pred = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Métriques de regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RSS = 11310517364.96\n",
      " RSE = 0.97\n",
      " R2 (Coefficient de determination) = 0.03\n"
     ]
    }
   ],
   "source": [
    "RSS=mean_squared_error(y_test, y_pred)*89\n",
    "print(\" RSS = %.2f\" % RSS)\n",
    "print(\" RSE = %.2f\" % (1-r2_score(y_test, y_pred)))\n",
    "print(\" R2 (Coefficient de determination) = %.2f\" % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etape 3 : GridSearchCV\n",
    "#### 10. Utilisation de la méthode GridSearchCV qui trouve facilement les valeurs optimales parmi les valeurs données pour les paramètres pertinents (activation, solver, learning_rate, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "mlp_gs = MLPRegressor(max_iter=3000)\n",
    "parameter_space = {\n",
    "    'activation': ['relu', 'identity', 'logistic', 'tanh'],\n",
    "    'solver': ['adam', 'lbfgs', 'sgd'],\n",
    "    'learning_rate': ['constant','adaptive', 'invscaling'],\n",
    "    'max_iter': [200, 2000, 3000], \n",
    "\n",
    "}\n",
    "#n_jobs=-1 , -1 sert à utiliser tous les cœurs de processeur disponibles.\n",
    "#cv = 10 est pour la validation croisée, ici cela signifie 10 fois la validation croisée stratifiée (K-fold)\n",
    "clf = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=10)\n",
    "\n",
    "#Apprentissage / Entrainement de modèle\n",
    "clf.fit(X, y)\n",
    "\n",
    "#Affichage des scores pour toutes les combinaisons (modèles) possibles \n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "\n",
    "i=0\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    i=i+1    \n",
    "print(\"Le nombre de combinaisons (modèles) possible est : %r \" % (i)) \n",
    "print(\" \\n \")\n",
    "print(\"|  Mean  |   Std   |  Combinaison / Modèle \")\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f    | %0.03f   | %r\" % (mean, std, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Affichage du meilleur modèle (hyperparamètres optimaux parmi les valeurs données). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Meilleurs paramètres trouvés : \\n', clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Test du meilleur modèle sur l'ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, y_pred = y_test , clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. Métriques de regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RSS=mean_squared_error(diabetes_y_test, diabetes_y_pred)*89\n",
    "print(\" RSS = %.2f\" % RSS)\n",
    "print(\" RSE = %.2f\" % (1-r2_score(diabetes_y_test, diabetes_y_pred)))\n",
    "print(\" R2 (Coefficient de determination) = %.2f\" % r2_score(diabetes_y_test, diabetes_y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
